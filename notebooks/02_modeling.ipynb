{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/processed/train_data.csv\", index_col = 0)\n",
    "df_val = pd.read_csv(\"../data/processed/validation_data.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep Classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepDataset():\n",
    "\n",
    "    def __init__(\n",
    "            self, df : pd.DataFrame, df_name : str, ohe_cutoff : int = 20, outliers : str = None, \n",
    "            target : str = \"isFraud\", imbalance : str = None, seed : int = 11, normalize : str = None,\n",
    "            cols_to_remove : list = [\"step\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"]):\n",
    "        \n",
    "        valid_names = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "        if df_name not in valid_names:\n",
    "            raise ValueError(f\"invalid df_name : {df_name}, use one of {valid_names}\")\n",
    "        \n",
    "        self.df_name = df_name\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.df_orginal = df.copy()\n",
    "\n",
    "        self.remove_columns(col_list = cols_to_remove)\n",
    "\n",
    "        self.encode_categoriacals(ohe_cutoff = ohe_cutoff)\n",
    "\n",
    "        if outliers:\n",
    "            self.replace_outliers(method = outliers, df_name = df_name)\n",
    "\n",
    "        self.x, self.y = None, None\n",
    "        self.split_x_y(target = target)\n",
    "\n",
    "        if imbalance and df_name == \"train\":\n",
    "            self.resampling(method = imbalance)\n",
    "\n",
    "        if normalize:\n",
    "            self.normalize_dataset(df_name = df_name, method = normalize)\n",
    "    \n",
    "    def remove_columns(self, col_list : list):\n",
    "        \"\"\"\n",
    "        This function drops a list of columns from \n",
    "        the dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df.drop(col_list, axis = 1, inplace = True)\n",
    "    \n",
    "    def encode_categoriacals(self, ohe_cutoff : int = 20, mappings_save_path : str = \"../data/other/\"):\n",
    "        \"\"\"\n",
    "        This function encodes the categorical columns so they can be feed into \n",
    "        a model. The parameter ohe_cutoff divides the columns into two lists. The \n",
    "        first list, containing less unique values per column than then the parameter, will store \n",
    "        the column names that will be one hot encoded. The second list, conataining more or \n",
    "        equal unique values per column than the parameter, will store the column names that will\n",
    "        be label encoded.\n",
    "        \"\"\"\n",
    "        mappings_save_path += f\"mappings_label_encoding_{self.df_name}.json\"\n",
    "\n",
    "        df_unq_vals = self.df.select_dtypes(include = \"object\").nunique().reset_index().rename({\"index\" : \"col_name\", 0 : \"n_unique_values\"}, axis = 1)\n",
    "        ohe_list = df_unq_vals.loc[df_unq_vals.n_unique_values < ohe_cutoff, \"col_name\"].tolist()\n",
    "        label_enc_list = df_unq_vals.loc[df_unq_vals.n_unique_values >= ohe_cutoff, \"col_name\"].tolist()\n",
    "\n",
    "        # one hot encoding \n",
    "        self.df = pd.get_dummies(data = self.df, columns = ohe_list, dtype = int)\n",
    "        \n",
    "        # label encoding\n",
    "        mappings_label_encoding = {}\n",
    "\n",
    "        for col in label_enc_list:\n",
    "            self.df[col] = self.df[col].astype(\"category\")\n",
    "            mappings_label_encoding[col] = dict(enumerate(self.df[col].cat.categories))\n",
    "\n",
    "            self.df[col] = self.df[col].cat.codes\n",
    "\n",
    "        with open(mappings_save_path, 'w') as f:\n",
    "            json.dump(mappings_label_encoding, f)\n",
    "    \n",
    "    def restore_data(self):\n",
    "        \"\"\"\n",
    "        This function restores the data to the original version before\n",
    "        replacing outliers.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = self.original_df.copy()\n",
    "    \n",
    "    def replace_outliers(self, method : str = \"z_score\", df_name : str = \"train\"):\n",
    "        \"\"\"\n",
    "        This function substitutes outliers with the border value \n",
    "        according to which method chosen.\n",
    "        \"\"\"\n",
    "        \n",
    "        valid_methods = [\"z_score\", \"iqr\", \"both\"]\n",
    "\n",
    "        if method not in valid_methods:\n",
    "            raise ValueError(f\"invalid method : {method}, use one of {valid_methods}\")\n",
    "        \n",
    "        \n",
    "        bounds_json = \"../data/other/outliers_bounds.json\"\n",
    "\n",
    "        if not os.path.exists(bounds_json):\n",
    "            outlier_bounds = {}\n",
    "        else:\n",
    "            with open(bounds_json, \"r\") as file:\n",
    "                outlier_bounds = json.load(file)\n",
    "        \n",
    "        \n",
    "        col_outliers = self.df.select_dtypes(include = \"float\").columns.tolist()\n",
    "\n",
    "        for col in col_outliers:\n",
    "\n",
    "            if df_name == \"train\":\n",
    "                mean = self.df[col].mean()\n",
    "                std = self.df[col].std()\n",
    "\n",
    "                self.df[\"z_score\"] = (self.df[col] - mean) / std\n",
    "                zscore_upper_bound = self.df[self.df.z_score <= 3].sort_values(by = col, ascending = False)[col].head(1).values[0]\n",
    "                zscore_lower_bound = self.df[self.df.z_score >= -3].sort_values(by = col)[col].head(1).values[0]\n",
    "                self.df.drop(\"z_score\", axis = 1, inplace = True)\n",
    "\n",
    "                q1 = self.df[col].quantile(0.25)\n",
    "                q3 = self.df[col].quantile(0.75)\n",
    "                iqr = q3 - q1 \n",
    "\n",
    "                iqr_upper_bound = q3 + 1.5 * iqr\n",
    "                iqr_lower_bound = q1 - 1.5 * iqr\n",
    "\n",
    "                outlier_bounds[col] = {\n",
    "                \"z_upper\": zscore_upper_bound, \"z_lower\": zscore_lower_bound,\n",
    "                \"iqr_upper\": iqr_upper_bound, \"iqr_lower\": iqr_lower_bound\n",
    "                }\n",
    "\n",
    "                with open(bounds_json, \"w\") as file:\n",
    "                    json.dump(outlier_bounds, file, indent=4)\n",
    "\n",
    "            else:\n",
    "                bounds = outlier_bounds.get(col)\n",
    "\n",
    "                if bounds is None:\n",
    "                    raise ValueError(f\"No stored bounds for column {col}.\")\n",
    "                \n",
    "                zscore_upper_bound = bounds[\"z_upper\"]\n",
    "                zscore_lower_bound = bounds[\"z_lower\"]\n",
    "                iqr_upper_bound = bounds[\"iqr_upper\"]\n",
    "                iqr_lower_bound = bounds[\"iqr_lower\"]\n",
    "\n",
    "            if method == \"both\":\n",
    "                upper_bound  = min(zscore_upper_bound, iqr_upper_bound)\n",
    "                lower_bound = max(zscore_lower_bound, iqr_lower_bound)\n",
    "            \n",
    "            elif method == \"z_score\":\n",
    "                upper_bound  = zscore_upper_bound\n",
    "                lower_bound = zscore_lower_bound\n",
    "            \n",
    "            elif method == \"iqr\":\n",
    "                upper_bound  = iqr_upper_bound\n",
    "                lower_bound = iqr_lower_bound\n",
    "        \n",
    "            if len(self.df[self.df[col] > upper_bound]) > 0:\n",
    "                    self.df.loc[self.df[col] > upper_bound, col] = upper_bound\n",
    "                \n",
    "            if len(self.df[self.df[col] < lower_bound]) > 0:\n",
    "                self.df.loc[self.df[col] < lower_bound, col] = lower_bound\n",
    "    \n",
    "    def split_x_y(self, target : str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        This function splits a dataframe in x (the dataframe containing the predictors) and\n",
    "        y (the series witht the target)\n",
    "        \"\"\"\n",
    "\n",
    "        predictors = self.df.columns.tolist()\n",
    "        predictors.remove(target)\n",
    "\n",
    "        self.x = self.df[predictors]\n",
    "        self.y = self.df[target]\n",
    "    \n",
    "    def resampling(self, method : str = \"smote\", sampling_strategy : float = 0.05):\n",
    "        \"\"\"\n",
    "        This function resamples the dataset based on the strategy chosen via the \n",
    "        method parameter \n",
    "        \"\"\"\n",
    "        valid_methods = [\"smote\", \"tomek\", \"both\"]\n",
    "\n",
    "        if method not in valid_methods:\n",
    "            raise ValueError(f\"invalid method : {method}, use one of {valid_methods}\")\n",
    "        \n",
    "        if method == \"smote\":\n",
    "            resampler = SMOTE(sampling_strategy = sampling_strategy, random_state = seed)\n",
    "        \n",
    "        elif method == \"tomek\":\n",
    "            resampler = TomekLinks(sampling_strategy = \"auto\")\n",
    "        \n",
    "        elif method == \"both\":\n",
    "            resampler = SMOTETomek(random_state = seed, sampling_strategy = sampling_strategy)\n",
    "        \n",
    "        self.x, self.y = resampler.fit_resample(self.x, self.y)\n",
    "\n",
    "    def normalize_dataset(self, df_name : str, method : str = \"standard\"):\n",
    "        \"\"\"\n",
    "        This function normalizes the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_methods = [\"standard\", \"min_max\"]\n",
    "\n",
    "        if method not in valid_methods:\n",
    "            raise ValueError(f\"invalid method : {method}, use one of {valid_methods}\")\n",
    "        \n",
    "        if df_name == \"train\":\n",
    "\n",
    "            if method == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            \n",
    "            elif method == \"min_max\":\n",
    "                scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "            \n",
    "            self.x = scaler.fit_transform(self.x)\n",
    "            self.x = pd.DataFrame(self.x, columns = scaler.feature_names_in_)\n",
    "\n",
    "            joblib.dump(scaler, f\"../models/scalers/{method}_scaler.pkl\")\n",
    "        \n",
    "        else:\n",
    "            scaler_path = f\"../models/scalers/{method}_scaler.pkl\"\n",
    "\n",
    "            if not os.path.exists(scaler_path):\n",
    "                raise FileNotFoundError(f\"No scaler found at {scaler_path}. Normalize training data first.\")\n",
    "            \n",
    "            else:\n",
    "                scaler = joblib.load(scaler_path)\n",
    "\n",
    "            self.x = scaler.transform(self.x)\n",
    "            self.x = pd.DataFrame(self.x, columns = scaler.feature_names_in_)\n",
    "\n",
    "    def return_x_y(self) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        This function returns the dataframe.\n",
    "        \"\"\"  \n",
    "\n",
    "        return self.x, self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Models Evaluation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicModelsEvaluation():\n",
    "\n",
    "    def __init__(\n",
    "            self, x_train : pd.DataFrame, y_train : pd.Series, x_val : pd.DataFrame, y_val : pd.Series, \n",
    "            df_eval : pd.DataFrame, imbalance : str, outliers : str, seed : int = 11):\n",
    "        self.x_train =  x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.df_eval = df_eval\n",
    "\n",
    "        self.imbalance = imbalance\n",
    "        self.outliers = outliers\n",
    "\n",
    "        with parallel_backend(\"threading\"):\n",
    "            self.evaluate_model(model = LogisticRegression(n_jobs = -1, random_state = seed), model_name = \"logistic regression\")\n",
    "            self.evaluate_model(model = RandomForestClassifier(n_jobs = -1, random_state = seed), model_name = \"random forest\")\n",
    "            self.evaluate_model(model = XGBClassifier(n_jobs = -1, random_state = seed), model_name = \"xgb\")\n",
    "        \n",
    "    def evaluate_model(self, model, model_name : str):\n",
    "        \"\"\"\n",
    "        This function first trains the model on the training data contained in the class, and \n",
    "        then evaluates it using both the training data and the validation data, also present in the\n",
    "        class. The results are then stored in the df_eval.\n",
    "        \"\"\"\n",
    "\n",
    "        # training the model\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "\n",
    "        # computing predictions\n",
    "        y_pred_train = model.predict(self.x_train)\n",
    "        y_pred_val = model.predict(self.x_val)\n",
    "        \n",
    "        # calculating metrics\n",
    "        accuracy_train = skm.accuracy_score(y_pred = y_pred_train, y_true = self.y_train)\n",
    "        recall_train = skm.recall_score(y_pred = y_pred_train, y_true = self.y_train, zero_division=0)\n",
    "        precision_train = skm.precision_score(y_pred = y_pred_train, y_true = self.y_train, zero_division=0)\n",
    "        f1_score_train = skm.f1_score(y_pred = y_pred_train, y_true = self.y_train, zero_division=0)\n",
    "\n",
    "        accuracy_val = skm.accuracy_score(y_pred = y_pred_val, y_true = self.y_val)\n",
    "        recall_val = skm.recall_score(y_pred = y_pred_val, y_true = self.y_val, zero_division=0)\n",
    "        precision_val = skm.precision_score(y_pred = y_pred_val, y_true = self.y_val, zero_division=0)\n",
    "        f1_score_val = skm.f1_score(y_pred = y_pred_val, y_true = self.y_val, zero_division=0)\n",
    "\n",
    "        # saving results\n",
    "        self.df_eval.loc[len(self.df_eval)] = {\n",
    "            \"imbalance\" : self.imbalance,\n",
    "            \"outliers\" : self.outliers,\n",
    "            \"model\" : model_name, \n",
    "            \"dataset\" : \"train\", \n",
    "            \"accuracy\" : accuracy_train,\n",
    "            \"recall\" : recall_train, \n",
    "            \"precision\" : precision_train, \n",
    "            \"f1_score\" : f1_score_train}\n",
    "\n",
    "        self.df_eval.loc[len(self.df_eval)] = {\n",
    "            \"imbalance\" : self.imbalance,\n",
    "            \"outliers\" : self.outliers,\n",
    "            \"model\" : model_name, \n",
    "            \"dataset\" : \"validation\", \n",
    "            \"accuracy\" : accuracy_val,\n",
    "            \"recall\" : recall_val, \n",
    "            \"precision\" : precision_val, \n",
    "            \"f1_score\" : f1_score_val}\n",
    "        \n",
    "    def return_df_eval(self):\n",
    "        \"\"\"\n",
    "        This function retuns the df eval.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and data preparation selction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute  = False \n",
    "# 43 min to compute \n",
    "\n",
    "if compute:\n",
    "    df_eval = pd.DataFrame(columns = [\"imbalance\", \"outliers\", \"model\", \"dataset\", \"accuracy\", \"recall\", \"precision\", \"f1_score\"])\n",
    "\n",
    "    outliers_methods = [None, \"z_score\", \"iqr\", \"both\"]\n",
    "    resampling_methods = [None, \"smote\", \"tomek\", \"both\"]\n",
    "\n",
    "    for outlier_method in outliers_methods:\n",
    "        for resampling_method in resampling_methods:\n",
    "            print(\"outlier_method :\", outlier_method, \"resampling_method :\", resampling_method)\n",
    "\n",
    "            outliers = outlier_method\n",
    "            imbalance = resampling_method\n",
    "\n",
    "            # Data Prep\n",
    "            data_prep_train = PrepDataset(df = df_train, df_name = \"train\", outliers= outliers, imbalance = imbalance)\n",
    "            x_train, y_train = data_prep_train.return_x_y()\n",
    "\n",
    "            data_prep_val = PrepDataset(df = df_val, df_name = \"validation\", outliers= outliers, imbalance = imbalance)\n",
    "            x_val, y_val = data_prep_val.return_x_y()\n",
    "\n",
    "            if not outliers:\n",
    "                outliers = \"none\"\n",
    "            if not imbalance:\n",
    "                imbalance = \"none\"\n",
    "\n",
    "            # Model Evaluation\n",
    "            classsic_model_eval = ClassicModelsEvaluation(\n",
    "                x_train = x_train, y_train = y_train, \n",
    "                x_val = x_val, y_val = y_val,\n",
    "                df_eval = df_eval , imbalance = imbalance, outliers = outliers)\n",
    "    \n",
    "    df_eval.to_csv(\"../data/evaluation/outliers_resampling.csv\")\n",
    "\n",
    "else:\n",
    "    df_eval = pd.read_csv(\"../data/evaluation/outliers_resampling.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imbalance</th>\n",
       "      <th>outliers</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.158247</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.263291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.157638</td>\n",
       "      <td>0.775449</td>\n",
       "      <td>0.262013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.157638</td>\n",
       "      <td>0.773134</td>\n",
       "      <td>0.261881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998849</td>\n",
       "      <td>0.155204</td>\n",
       "      <td>0.768072</td>\n",
       "      <td>0.258228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.151552</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.254341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998846</td>\n",
       "      <td>0.152161</td>\n",
       "      <td>0.766871</td>\n",
       "      <td>0.253936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.149117</td>\n",
       "      <td>0.800654</td>\n",
       "      <td>0.251411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>smote</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998295</td>\n",
       "      <td>0.185027</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.218862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smote</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>0.190505</td>\n",
       "      <td>0.242448</td>\n",
       "      <td>0.213361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>both</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.186245</td>\n",
       "      <td>0.232523</td>\n",
       "      <td>0.206827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imbalance outliers          model     dataset  accuracy    recall  \\\n",
       "39     tomek  z_score  random forest  validation  0.998857  0.158247   \n",
       "15     tomek     none  random forest  validation  0.998853  0.157638   \n",
       "3       none     none  random forest  validation  0.998853  0.157638   \n",
       "27      none  z_score  random forest  validation  0.998849  0.155204   \n",
       "29      none  z_score            xgb  validation  0.998853  0.151552   \n",
       "17     tomek     none            xgb  validation  0.998846  0.152161   \n",
       "41     tomek  z_score            xgb  validation  0.998853  0.149117   \n",
       "35     smote  z_score            xgb  validation  0.998295  0.185027   \n",
       "11     smote     none            xgb  validation  0.998186  0.190505   \n",
       "47      both  z_score            xgb  validation  0.998156  0.186245   \n",
       "\n",
       "    precision  f1_score  \n",
       "39   0.783133  0.263291  \n",
       "15   0.775449  0.262013  \n",
       "3    0.773134  0.261881  \n",
       "27   0.768072  0.258228  \n",
       "29   0.790476  0.254341  \n",
       "17   0.766871  0.253936  \n",
       "41   0.800654  0.251411  \n",
       "35   0.267841  0.218862  \n",
       "11   0.242448  0.213361  \n",
       "47   0.232523  0.206827  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_10 = df_eval[df_eval.dataset == \"validation\"].sort_values(by = \"f1_score\", ascending = False).head(10)\n",
    "df_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imbalance</th>\n",
       "      <th>outliers</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>train</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.158247</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.263291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>train</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.996695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.157638</td>\n",
       "      <td>0.775449</td>\n",
       "      <td>0.262013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>train</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.997391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.157638</td>\n",
       "      <td>0.773134</td>\n",
       "      <td>0.261881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>train</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.997217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>random forest</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998849</td>\n",
       "      <td>0.155204</td>\n",
       "      <td>0.768072</td>\n",
       "      <td>0.258228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.998897</td>\n",
       "      <td>0.165072</td>\n",
       "      <td>0.894439</td>\n",
       "      <td>0.278708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>none</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.151552</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.254341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.165072</td>\n",
       "      <td>0.813894</td>\n",
       "      <td>0.274476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tomek</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998846</td>\n",
       "      <td>0.152161</td>\n",
       "      <td>0.766871</td>\n",
       "      <td>0.253936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.998885</td>\n",
       "      <td>0.159854</td>\n",
       "      <td>0.875238</td>\n",
       "      <td>0.270334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tomek</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.149117</td>\n",
       "      <td>0.800654</td>\n",
       "      <td>0.251411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>smote</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.980614</td>\n",
       "      <td>0.603483</td>\n",
       "      <td>0.982742</td>\n",
       "      <td>0.747773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>smote</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998295</td>\n",
       "      <td>0.185027</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.218862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>smote</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.980611</td>\n",
       "      <td>0.608721</td>\n",
       "      <td>0.974560</td>\n",
       "      <td>0.749375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smote</td>\n",
       "      <td>none</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>0.190505</td>\n",
       "      <td>0.242448</td>\n",
       "      <td>0.213361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>both</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>train</td>\n",
       "      <td>0.982352</td>\n",
       "      <td>0.610355</td>\n",
       "      <td>0.979947</td>\n",
       "      <td>0.752204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>both</td>\n",
       "      <td>z_score</td>\n",
       "      <td>xgb</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.186245</td>\n",
       "      <td>0.232523</td>\n",
       "      <td>0.206827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imbalance outliers          model     dataset  accuracy    recall  \\\n",
       "38     tomek  z_score  random forest       train  0.999995  0.996347   \n",
       "39     tomek  z_score  random forest  validation  0.998857  0.158247   \n",
       "14     tomek     none  random forest       train  0.999996  0.996695   \n",
       "15     tomek     none  random forest  validation  0.998853  0.157638   \n",
       "2       none     none  random forest       train  0.999997  0.997391   \n",
       "3       none     none  random forest  validation  0.998853  0.157638   \n",
       "26      none  z_score  random forest       train  0.999996  0.997217   \n",
       "27      none  z_score  random forest  validation  0.998849  0.155204   \n",
       "28      none  z_score            xgb       train  0.998897  0.165072   \n",
       "29      none  z_score            xgb  validation  0.998853  0.151552   \n",
       "16     tomek     none            xgb       train  0.998873  0.165072   \n",
       "17     tomek     none            xgb  validation  0.998846  0.152161   \n",
       "40     tomek  z_score            xgb       train  0.998885  0.159854   \n",
       "41     tomek  z_score            xgb  validation  0.998853  0.149117   \n",
       "34     smote  z_score            xgb       train  0.980614  0.603483   \n",
       "35     smote  z_score            xgb  validation  0.998295  0.185027   \n",
       "10     smote     none            xgb       train  0.980611  0.608721   \n",
       "11     smote     none            xgb  validation  0.998186  0.190505   \n",
       "46      both  z_score            xgb       train  0.982352  0.610355   \n",
       "47      both  z_score            xgb  validation  0.998156  0.186245   \n",
       "\n",
       "    precision  f1_score  \n",
       "38   1.000000  0.998170  \n",
       "39   0.783133  0.263291  \n",
       "14   1.000000  0.998345  \n",
       "15   0.775449  0.262013  \n",
       "2    1.000000  0.998694  \n",
       "3    0.773134  0.261881  \n",
       "26   1.000000  0.998607  \n",
       "27   0.768072  0.258228  \n",
       "28   0.894439  0.278708  \n",
       "29   0.790476  0.254341  \n",
       "16   0.813894  0.274476  \n",
       "17   0.766871  0.253936  \n",
       "40   0.875238  0.270334  \n",
       "41   0.800654  0.251411  \n",
       "34   0.982742  0.747773  \n",
       "35   0.267841  0.218862  \n",
       "10   0.974560  0.749375  \n",
       "11   0.242448  0.213361  \n",
       "46   0.979947  0.752204  \n",
       "47   0.232523  0.206827  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_10_all = df_eval.loc[[num for x in df_top_10.index.tolist() for num in (x -1, x)]]\n",
    "df_top_10_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the possibilities we have a rough idea as to which model, outlier strategy, and imbalance stratgey to use going forward. \n",
    "\n",
    "While the random forest has the best results it is very slow to train and the differences in results on validation set are marginal, so keeping this in mind I am going to move forward with the XGBoost Classifier for further analysis.\n",
    "\n",
    "For dealing with outliers we can rule out IQR and both, while depending on the cases none and Z score have producedd good results.\n",
    "\n",
    "Regarding the imbalance in the target I don't feel confindent exclduing any of the methodologies as I believe they require more resarch before reaching a definitive conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis of XGboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
